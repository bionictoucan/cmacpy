{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Using the Fully-connected Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following will demonstrate how to use the ``FCLayer`` object from the\n``neural_network_layers`` script. This is a highly customisable\nfully-connected layer for use in construction of bespoke neural networks. We\nwill start by importing the object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ntorch.manual_seed(12)\nimport torch.nn as nn\nfrom cmacpy.nn.neural_network_layers import FCLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An ``FCLayer`` object can take a variety of arguments during construction (for\nmore detail see `layers`). Neural network layers in general are\nconstructed using two or three objects: a linear function, a normalisation\n(optional) and a non-linear function. In a fully-connected layer, the linear\nfunction takes the form a `PyTorch <https://pytorch.org/>`_ ``nn.Linear``\nmodule which maps the input $x$ to the output $y$ via the\nfollowing equation\n\n\\begin{align}y = \\Theta x + b\\end{align}\n\nwhere $\\Theta$ is known as the learnable parameters or weights of the\nlayer -- it is a matrix of numbers which multiplies the vector of inputs\n$x$. These are the parameters that are optimised to give the best\ntransformation of the data. $b$ is an optional, learnable bias of the\nlinear transformation. The two main arguments for the linear transformation\n(and thus of the ``FCLayer`` object) are ``in_nodes`` and ``out_nodes``: this\nis the dimensionality of the input $x$ and the dimensionality of the\noutput $y$. This is used to construct the learnable parameters\n$\\Theta$. ``in_nodes`` and ``out_nodes`` are integers.\n\nThe non-linear function (often referred to as the activation) that is part of\na neural network layer can be set through the ``activation`` keyword argument.\nIts default value ``\"relu\"`` uses the rectified linear unit non-linearity but\nother options exist [#f1]_.\n\n## Setting up an ``FCLayer``\nTo construct the layer we do the following:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fclayer = FCLayer(3, 5)\n\nprint(fclayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This create a fully-connected layer with input dimension 3, output dimension\n5, no bias and the ReLU non-linearity. This is shown in the figure below.\n\n<img src=\"file://../images/fclayer.png\" width=\"400\" align=\"center\">\n\nThe input $x$ consists of three numbers: $x_{1}, x_{2}, x_{3}$.\nThe output $y$ consists of five numbers: $y_{1}, y_{2}, y_{3},\ny_{4}, y_{5}$. The arrows indicate that each of the three inputs has a part to\nplay in forming each of the five outputs &mdash; this is where the name\n\"fully-connected\" comes from, every input is connected to every output. Each\nof these arrows also represent an element of the learnable parameters\n$\\Theta$ e.g. the value of $y_{1}$ is obtained via a linear\ncombination of the input values multiplied by the associated weight for that\nconnection. In more mathematical terms, $\\Theta$ is a matrix consisting\nof each weight ordered by the connections between the inputs and outputs.\nFollowing our example, if we label the weight from $x_{1}$ to\n$y_{1}$ as $\\theta_{11}$, the weight from $x_{2}$ to\n$y_{1}$ as $\\theta_{12}$ and so on and so forth then the matrix of\nweights can be written as\n\n\\begin{align}\\Theta = \\begin{bmatrix}\n                     \\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n                     \\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n                     \\theta_{31} & \\theta_{32} & \\theta_{33} \\\\\n                     \\theta_{41} & \\theta_{42} & \\theta_{43} \\\\\n                     \\theta_{51} & \\theta_{52} & \\theta_{53}\n              \\end{bmatrix}\\end{align}\n\nThe output $y$ can then be calculated via matrix multiplication of\n$\\Theta$ and $x$ (plus the potential addition of the bias).\n\nThe outputs are then operated on by the activation to produce the final output\nof the ``FCLayer``. In this example we use the ReLU activation which will\nreturn the value passed to the function if the value is positive and will\nreturn zero otherwise.\n\nThis is equivalent to defining a ``nn.Sequential`` object as follows\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fcseq = nn.Sequential(nn.Linear(3, 5, bias=False), nn.ReLU(inplace=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main idea is to have a nicer looking way of representing these layers\n(especially when the networks get really deep). Each element of the\n``FCLayer`` object can be accessed via class attributes ``.lin``` for the\nlinear function and ``.act`` for the non-linearity e.g.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer.lin, fclayer.act)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dummy input can be created to show that the objects ``fclayer`` and\n``fcseq`` are equivalent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randint(\n    10, (3,)\n).float()  # randomly sample 3 integers from the range [0,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The default random initialisation for ``nn.Linear`` objects samples a\n    uniform distribution bounded by $\\pm 1/\\sqrt{dim(x)}$ for *reasons*\n    (I really don't know this answer to why). Due to the nature of the random\n    sampling, the easiest way to compare these two examples is to initialise\n    them with the same numbers. Below we initialise them with 0.5</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nn.init.constant_(fclayer.weight, 0.5)\nnn.init.constant_(fcseq[0].weight, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the output is calculated for passing the dummy inputs to the different\nformulations of the layer.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The layers can be applied to inputs in the same manner as functions.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer(dummy_input), fcseq(dummy_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further, we can check how the layer we have defined interacts with gradient\nbackpropagation by creating a dummy desired output and a loss function that we\nwant to use to optmise our layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = fclayer(dummy_input)\noutput_seq = fcseq(dummy_input)\ndummy_output = torch.ones(5)\nloss = torch.nn.functional.mse_loss(output, dummy_output)\nloss_seq = torch.nn.functional.mse_loss(output_seq, dummy_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above code assigns the output of the ``FCLayer`` applied to the dummy\ninput to the variable ``output`` alongside the output of the\n``nn.Sequential``'s output being labelled ``output_seq``. Then a fake desired\noutput of ones in each dimension is created (``dummy_output``) and the mean\nsquared error is calculated for each to see how close the layer gets to this\ndesired output (``loss`` and ``loss_seq``, respectively following the naming\nconventions for the outputs). The gradient of the loss with respect to each\nweight is then calculated doing the following\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss.backward()\nloss_seq.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These calculated gradients can then be accessed from the original objects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer.weight.grad, fcseq[0].weight.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen, both the outputs and the gradients calculated via\nbackpropagation are identical whether using this customisable block or using\nraw PyTorch so hopefully having a tidier wrapper to keep everything in is\nuseful!\n\nFor more advanced use of the ``FCLayer`` object, the interested reader is\nreferred to `fclayer_adv_exa`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. [#f1] These are the most commonly-used non-linearities but implementations\n         of other can be added when needed. Also, a custom non-linearity\n         function being addable is being considered (I only thought of it when\n         writing this document).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}