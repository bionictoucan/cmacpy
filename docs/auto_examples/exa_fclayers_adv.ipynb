{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Advanced Usage of the Fully-connected Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following makes the assumption that you are already comfortable with the\ncontents of `fclayer_exa`.\n\nAs always, the layer must be imported\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cmacpy.nn.neural_network_layers import FCLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalisation\nA third, often vital, operation part of any neural network layer is the\nnormalisation layer. Normalisation across batches of data (known as \"batch\nnormalisation\") is useful in reducing the dynamic range of your batches at the\nexpense of having a learnable mean and standard deviation. In the grand scheme\nof having millions of fitted parameters in your network, having a couple more\ndue to batch normalisation is preferable over the network taking some time to\nlearn the data distribution.\n\nNormalisation can be added to an ``FCLayer`` object using the keyword argument\n``normalisation``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fclayer = FCLayer(3, 5, normalisation=\"batch\", initialisation=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Valid options here are ``\"batch\"`` and ``\"instance\"`` to add batch\nnormalisation and instance normalisation respectively but batch normalisation\nis typically the most useful. The layout of the layer is then\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The normalisation operation itself can be accessed via the ``.norm`` attribute\nof the class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer.norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropout\nAnother technique that can be added to neural network layers is called\n\"dropout\". Dropout will assign a probability to each of the connections in a\nfully-connected layer and randomly not use those transformations during\nlearning iterations. This is employed in deeper networks to avoid overfitting\nwith the intuition that the model only being able to use some of its\nparameters while training will result in a more general model. An example of\nthis is shown in the image below: using our same fully-connected layer defined\nabove, there is a probability that an input won't be used to calculate an\noutput. Instead the output will be estimated from a subset of the parameters.\nThis is indicated by the dashed lines e.g. without dropout the output\n$y_{1}$ is calculated by combining the inputs $x_{1}, x_{2},\nx_{3}$, however in this example the connection from $x_{2}$ to\n$y_{1}$\nis dropped meaning $y_{1}$ is now calculated from just $x_{1},\nx_{3}$\n\n<img src=\"file://../images/fclayer_withdropout.png\" width=\"400\" align=\"center\">\n\nThere are two keyword arguments associated with dropout here: ``use_dropout``\nwhich can be set to ``True`` if the user wishes to include dropout in the\nlayer and ``dropout_prob`` which is the probability assigned to each\nconnection of whether or not it will be dropped (the default value for this is\n0.5, 50% chance of the connection not being used).\n\nFor example if we wanted to make an ``FCLayer`` which uses dropout and each\nconnection has a 30% chance of being dropped this would be formulated like so\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fclayer = FCLayer(\n    3, 5, normalisation=\"batch\", initialisation=None, use_dropout=True, dropout_prob=0.3\n)\n\nprint(fclayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dropout operation can then be accessed via the ``.dropout`` attribute of\nthe class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fclayer.dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialisation\nThe last thing that can be added to the ``FCLayer`` is a different\ninitialisation scheme for the weights. So far, we have been setting the\n``initalisation`` keyword argument to ``None`` which causes the learnable\nparameters to be initialised using the standard method discussed in\n`fclayer_exa`. Other initialisation methods can be employed through this\nkwarg, namely He initialisation and Xavier initialisation.\n\nHe initialisation (the default), was used for `the first deep learning\nalgorithm that surpassed human level classification percentage\n<https://arxiv.org/abs/1502.01852>`_, bases the initialisation on drawing\nrandom samples from a normal distribution with mean zero and standard\ndeviation inversely proportional to the number of connections in a layer and\nproportional to a value describing the effect the non-linear function has on\nthe variance of the output distribution (for a mathematical derivation of this\nsee the paper above) and using these as the starting points for the weights.\n\nXavier initialisation is a special case of He initialisation where it is\nassumed that the non-linearity does not contribute to the variance of the\noutput distribution of the layer.\n\nInitialisation using both schemes is shown below\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fclayer = FCLayer(3, 5, normalisation=\"batch\", initialisation=\"he\")\n\nprint(fclayer.weight)\n\nfclayer = FCLayer(3, 5, normalisation=\"batch\", initialisation=\"xavier\")\n\nprint(fclayer.weight)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}